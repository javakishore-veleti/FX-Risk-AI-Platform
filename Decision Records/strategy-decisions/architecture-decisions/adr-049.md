# ADR-049: Versioning Strategy for Prompts, Models, and Embeddings

**Status**: Accepted  
**Date**: YYYY-MM-DD  
**Deciders**: Chief Architect, MLOps Lead, Prompt Engineering Lead  
**Context Level**: AI Development Lifecycle, Reproducibility, Governance

## Context

In AI platforms, reproducibility and traceability are essential. However, without a formal versioning strategy:

- Prompts evolve ad hoc, breaking behavior or expectations  
- Embeddings are refreshed without lineage, impacting retrieval accuracy  
- Model promotions are done without audit trails or rollback capability  
- Teams cannot confidently compare or debug outputs over time

A unified versioning approach is needed across all core AI artifacts.

## Decision

Adopt a **Standardized Versioning Strategy** for prompts, models, and embeddings to enable traceability, reproducibility, and audit compliance.

### Versioning Scope and Strategy:

1. **Prompts**  
   - Treated as code; stored in version-controlled repo (e.g., Git)  
   - Follow semantic versioning (`v1.2.0`)  
   - Include metadata: task type, last updated, compatible models, author  
   - Changes require peer review and prompt regression tests

2. **Models**  
   - Each fine-tuned model has a version ID (e.g., `model-fx-v3.1.0`)  
   - Metadata includes training dataset snapshot, evaluation metrics, promotion notes  
   - Registered in ML model registry (e.g., MLflow, SageMaker Model Registry)

3. **Embeddings**  
   - Each embedding index has a version tied to:  
     - Embedding model used (e.g., `bge-small-v1`)  
     - Document set or ingestion batch ID  
     - Chunking strategy  
   - Indexed via vector DB with namespace/version tagging

4. **Changelogs and Rollbacks**  
   - All changes logged with user, timestamp, rationale  
   - Fallback paths documented (e.g., revert to previous prompt or model)

## Rationale

- **CIO Alignment**: Guarantees explainability and supports governance/audit reviews  
- **CTO Alignment**: Enables safe iteration and team collaboration across features  
- **Chief Architect Alignment**: Establishes infrastructure for long-term platform maturity

## Consequences

### Benefits

- Makes debugging and incident response faster and more transparent  
- Avoids silent regressions when updating AI components  
- Supports scenario testing and A/B evaluations  
- Facilitates alignment with ML governance and compliance policies

### Trade-Offs

- Requires discipline in tracking changes and metadata accuracy  
- Small teams may find versioning overhead during prototyping  
- Coordination needed between prompt, model, and data teams

## Alternatives Considered

- Manual tracking in spreadsheets or inline comments  
  → Not scalable or reliable for growing AI platforms.

- No versioning for prompts or embeddings  
  → Leads to silent failures, lost context, and lack of reproducibility.

## Lifecycle and Governance

- Prompt registry owned by Prompt Engineering Guild  
- Model registry enforced via CI/CD and MLOps workflows  
- Embedding updates require index lineage tracking and archiving  
- Version alignment required before production deployment

## Tags

`#versioning` `#prompts` `#models` `#embeddings` `#traceability` `#ai-governance` `#mlops` `#semantic-versioning` `#reproducibility`
