# ADR-032: Managed Model Lifecycle with Versioning and Registry

**Status**: Accepted  
**Date**: YYYY-MM-DD  
**Deciders**: Chief Architect, ML Engineering Lead, Data Science Governance Board  
**Context Level**: AI/ML Platform Architecture and Governance

## Context

As organizations deploy machine learning models into production, they often encounter challenges related to:

- Lack of visibility into which model version is running  
- Inconsistent tracking of training parameters, features, and datasets  
- Difficulties in debugging, auditing, or rolling back models  
- Duplication of effort when reusing models across environments or use cases

An unmanaged model lifecycle results in compliance risk, reproducibility issues, and degraded business trust.

## Decision

Adopt a **Managed Model Lifecycle Framework** with the following core components:

1. **Model Registry**  
   - Centralized service (e.g., MLflow Model Registry, SageMaker Model Registry, Azure ML)  
   - Tracks models with metadata: version, owner, input/output schema, metrics, audit trail

2. **Model Versioning Policy**  
   - Semantic versioning (`v1.0.0`) enforced for production models  
   - Version bump criteria tied to code, data, or performance changes

3. **Lifecycle States**  
   - Models move through states: `Staging` → `Validated` → `Production`  
   - Automated or manual approval gates

4. **Artifact Storage**  
   - Models stored in cloud blob (e.g., S3, Azure Blob) with integrity checksums and expiry tagging

5. **Metadata Integration**  
   - Every deployed model must log metadata: training config, dataset hash, evaluation metrics, timestamp, creator

## Rationale

- **CIO Alignment**: Supports model explainability, traceability, and audit-readiness  
- **CTO Alignment**: Enables safe deployment, rollback, and cross-environment consistency  
- **Chief Architect Alignment**: Provides modular governance and clean contract interfaces for models

## Consequences

### Benefits

- Enables safe A/B testing, rollback, and blue/green deployments  
- Makes models first-class citizens in SDLC with clear lifecycle visibility  
- Improves reuse, especially for foundation models, embeddings, and shared preprocessing  
- Facilitates compliance audits and incident response (model lineage)

### Trade-Offs

- Requires new tooling and training for data scientists and ML engineers  
- Enforces discipline that may slow down early experimentation  
- Storage and registry maintenance costs increase with model volume

## Alternatives Considered

- Manual model tracking via spreadsheets or notebooks  
  → Inconsistent, unscalable, non-auditable.

- Auto-deploy “latest” model from pipeline  
  → Dangerous for production; no rollback or traceability.

## Lifecycle and Governance

- Model registry governed by ML platform team  
- Training pipelines must automatically log version and metadata  
- Only validated versions can be promoted to production  
- Monthly cleanup of stale or unused models based on TTL and usage metrics

## Tags

`#mlops` `#model-versioning` `#model-registry` `#ai-governance` `#model-lifecycle` `#auditability` `#reproducibility`
