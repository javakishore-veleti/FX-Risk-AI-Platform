# ADR-005: Adoption of AWS Bedrock for LLM Prompting

**Status**: Accepted  
**Date**: YYYY-MM-DD  
**Deciders**: Chief Architect, CTO, Head of AI/ML  
**Context Level**: AI Architecture and Cloud Service Governance

## Context

As enterprise platforms begin to integrate large language models (LLMs) into critical workflows—such as case summarization, exception classification, and response drafting—there is a growing need for:

- Secure, compliant, and audit-ready LLM integration  
- Model diversity and future-proofing (e.g., Claude, Titan, Jurassic, Llama)  
- Cost control, usage visibility, and centralized governance  
- Avoidance of vendor lock-in via foundation model abstraction

While direct calls to OpenAI, Anthropic, or other APIs are possible, they raise concerns around:

- Data residency and compliance boundaries  
- IAM integration, monitoring, and logging gaps  
- Limited traceability or contextual isolation for regulated industries

## Decision

Adopt **AWS Bedrock** as the enterprise standard interface for large language model (LLM) invocation in production-grade workflows.

## Rationale

- **CIO Alignment**: Ensures data privacy, auditability, and compliance within cloud policy boundaries  
- **CTO Alignment**: Centralizes model access behind a consistent SDK and cost control surface  
- **Chief Architect Alignment**: Provides abstraction over model providers, simplifies prompt governance, and integrates natively with IAM and observability tooling

## Consequences

### Benefits

- **Multi-Model Access**: Support for multiple LLMs via a common API (Claude, Mistral, Titan, etc.)  
- **Native Cloud Security**: Integrates with AWS IAM, VPC endpoints, and CloudTrail  
- **Cost and Quota Management**: Enables per-role quotas, usage reports, and spend governance  
- **Service Composition**: Easily embedded in Step Functions, Lambdas, and Bedrock Agents

### Trade-Offs

- **Regional Availability**: Bedrock is only available in select AWS regions as of now  
- **New Abstractions**: Requires onboarding teams to AWS SDKs and Bedrock service limits  
- **Less Experimental Flexibility**: Some open-source models or parameter tuning capabilities may be more constrained than raw API usage

## Alternatives Considered

- **Direct OpenAI API usage**  
  Offers fastest access to innovation but lacks cloud-native governance and introduces data egress risks.

- **Custom-hosted models on SageMaker or ECS**  
  Enables deeper tuning but adds maintenance, scaling, and ops overhead.

- **Azure OpenAI**  
  Good compliance alignment, but may not fit organizations committed to AWS-first architectures.

## Lifecycle and Governance

- All prompt orchestration must integrate with Bedrock unless explicitly exempted for R&D  
- Each LLM usage must log `prompt_id`, `model_id`, `caller`, and `context_source`  
- Prompts must be versioned and governed through structured templates (e.g., YAML or Markdown with frontmatter metadata)

## Tags

`#llm`, `#aws-bedrock`, `#prompt-orchestration`, `#ai-infrastructure`, `#model-abstraction`, `#cloud-governance`
