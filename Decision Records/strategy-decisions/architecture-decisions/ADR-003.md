# ADR-003: Selection of Apache Spark for Batch Data Pipelines

**Status**: Accepted  
**Date**: YYYY-MM-DD  
**Deciders**: Chief Architect, Head of Data Engineering, CTO  
**Context Level**: Enterprise Data & Analytics Architecture

## Context

Enterprises often require high-throughput data processing to support forecasting, performance analytics, reconciliation, and model training workflows. These pipelines:

- Process terabytes of structured, semi-structured, and time-series data  
- Demand scalable join, aggregation, and enrichment capabilities  
- Must support batch windows that align with business SLAs  
- Require data lineage, schema evolution handling, and reproducibility

A scalable and mature engine is needed to orchestrate these offline pipelines in a cloud-native, modular, and governance-compliant manner.

## Decision

Adopt **Apache Spark** as the core engine for all batch data pipelines and offline feature generation. Spark will be run on managed services such as AWS Glue, EMR, Azure Synapse, or GCP Dataproc depending on cloud strategy and team expertise.

### Primary Workloads

- Forecasting model training and scoring  
- SLA violation trend computation  
- Rule-based exception generation  
- Historical usage and volume pattern analysis  
- Batch ETL for reporting and dashboarding

## Rationale

- **CIO Alignment**: Provides scalable processing to power enterprise metrics and AI-assisted planning without bottlenecks  
- **CTO Alignment**: Leverages open standards (SparkSQL, PySpark), with managed runtimes across clouds and robust connector support  
- **Chief Architect Alignment**: Aligns with BDAT architecture for analytics, ensures reproducibility, and supports modular job orchestration

## Consequences

### Benefits

- **Scalability**: Horizontal compute scaling across large datasets  
- **Flexibility**: Supports SQL, Python (PySpark), and Java APIs  
- **Ecosystem**: Broad support for Parquet, Delta Lake, Iceberg, JDBC, and Glue Catalog  
- **Governance**: Supports schema enforcement, job audit logging, and fine-grained lineage tracking

### Trade-Offs

- **Startup Latency**: Jobs may take longer to initialize compared to lightweight tools  
- **Cost Profile**: Resource-intensive if not tuned or scheduled efficiently  
- **Complexity**: Requires experienced Spark developers and proper testing harnesses

## Alternatives Considered

- **dbt + SQL-only pipelines**  
  Simpler for transformations but lacks ML, feature engineering, and scale for large joins.

- **Pandas / Dask**  
  Python-native, easier for prototyping but unsuitable for production-scale batch ETL.

- **Flink or Streaming-First**  
  Great for low-latency use cases but not optimized for long-running batch aggregation windows.

## Lifecycle and Governance

- All batch pipelines must include logging, schema checks, and SLA-compliant scheduling  
- Jobs must be versioned and deployed via infrastructure pipelines (e.g., Airflow, Step Functions, MWAA)  
- Catalog and data governance must be integrated via metadata tags and Glue Catalog (or equivalent)

## Tags

`#batch-processing`, `#apache-spark`, `#data-pipelines`, `#forecasting`, `#ml-etl`, `#analytics`
