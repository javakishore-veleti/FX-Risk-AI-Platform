# ADR-038: Event-Driven Architecture for ML Model Invocation

**Status**: Accepted  
**Date**: YYYY-MM-DD  
**Deciders**: Chief Architect, ML Platform Lead, Integration Engineering Lead  
**Context Level**: Real-Time Inference, Event Streaming, System Architecture

## Context

In many operational workflows, such as dispute resolution, trade validation, or SLA prediction, ML models need to be invoked in near real-time or asynchronously in response to business events. Traditional synchronous REST API invocation of models introduces tight coupling, latency sensitivity, and limited scalability.

Modern AI systems benefit from **event-driven architectures**, where model inference is triggered by business events (e.g., new claim, anomaly detection, user action) published via queues or streaming platforms.

## Decision

Adopt an **Event-Driven Architecture (EDA)** for model invocation across applicable AI use cases.

### Key Design Elements:

1. **Event Bus**  
   - Use messaging infrastructure like Kafka, Amazon EventBridge, or Azure Event Grid  
   - Standardize message schema for events such as `ClaimCreated`, `TradeEnriched`, `ForecastTrigger`

2. **Model Worker Services**  
   - Stateless services subscribed to relevant events  
   - Perform feature lookup, run inference, and emit results (e.g., `RiskScored`, `ResolutionPredicted`)

3. **Async Results Delivery**  
   - Output published to downstream topics, stored in S3/Blob, or invoked via webhook  
   - Enables chaining multiple AI models with orchestration policies

4. **Traceable Event Headers**  
   - Include `trace_id`, `event_id`, and `timestamp` in every message  
   - Supports full observability across pipelines

5. **Retry and DLQ**  
   - All consumers implement retry logic and dead letter queue (DLQ) handling for failed inference

## Rationale

- **CIO Alignment**: Enables responsive, scalable, and auditable decision flows  
- **CTO Alignment**: Supports decoupled, horizontally scalable microservices  
- **Chief Architect Alignment**: Encourages modular, low-latency model integration aligned with enterprise messaging

## Consequences

### Benefits

- Reduces latency between business action and model insight  
- Enables orchestration of multiple ML components without central control  
- Improves fault tolerance and throughput under load  
- Simplifies integration into existing event-driven applications

### Trade-Offs

- Higher complexity in managing schemas, ordering, and idempotency  
- Monitoring and debugging asynchronous flows can be harder than REST  
- Risk of event backlog under high load without proper scaling

## Alternatives Considered

- Synchronous model invocation via REST APIs  
  → Simple to implement but lacks elasticity and can block upstream services.

- Scheduled batch inference  
  → Suitable for low-frequency use cases but not real-time responsiveness.

## Lifecycle and Governance

- Platform Integration team defines and maintains event schema registry  
- Consumers deployed using containerized pipelines with autoscaling policies  
- DLQ monitored by SRE team with alerting on failure spikes  
- Monthly reviews of topic throughput, latency, and error rates

## Tags

`#event-driven` `#ml-inference` `#architecture` `#kafka` `#streaming` `#async-models` `#microservices` `#traceability` `#resilience`
